{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import torchio as tio\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import PchipInterpolator, interp1d\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import statsmodels.stats.inter_rater as irr\n",
    "from statsmodels.stats.inter_rater import aggregate_raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_data_pths = [\n",
    "                      './datasets/LN_classify/Fudan_HN_LN_22-23_all/Fudan_HN_LN_20231204_patches',\n",
    "                      ]\n",
    "\n",
    "external_data_pths = ['./datasets/LN_classify/Fudan_HN_LN_22-23_all/CGMH/CGMH_2024_patches',\n",
    "                      './datasets/LN_classify/Fudan_HN_LN_22-23_all/TCGA/TCGA-HNSC_selected_patches',\n",
    "                      './datasets/LN_classify/Fudan_HN_LN_22-23_all/CGMH_Oral/CGMH_Oral_patches'\n",
    "                      ]\n",
    "\n",
    "name_to_sad = {}\n",
    "for data_idx, crop_pth in enumerate(internal_data_pths + external_data_pths):\n",
    "    cropfile = osp.join(crop_pth, \"cropping_list.csv\")\n",
    "    df = pd.read_csv(cropfile)\n",
    "    for idx, row in df.iterrows():\n",
    "        name_to_sad[row['basename'].lower()] = float(row['recist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MIN_SAD = 5.0\n",
    "# _MAX_SAD = {'EENT': 21.8850, 'CGMH': 11.25, 'TCGA': 9999, 'CGMH-Oral': 9999, 'Dev': 9999} # q50 limit\n",
    "_MAX_SAD = {'EENT': 9999, 'CGMH': 9999, 'TCGA': 9999, 'CGMH-Oral': 9999, 'Dev': 9999} # no limit\n",
    "renji_ln_count = {}\n",
    "renji_ln_rm = {}\n",
    "cgmh_ln_count = {}\n",
    "cgmh_ln_rm = {}\n",
    "tcga_ln_count = {}\n",
    "tcga_ln_rm = {}\n",
    "\n",
    "# renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/Fudan_HN_LN_20231204/2023-08_LN_data_reorganize_external\"\n",
    "renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/Fudan_HN_LN_20231204_patches/Ext\"\n",
    "# cgmg_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/CGMH/CGMH_2024_reoriented\"\n",
    "cgmh_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/CGMH/CGMH_2024_patches\"\n",
    "# tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/TCGA/TCGA-HNSC_selected_reori_labeled\"\n",
    "tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/TCGA/TCGA-HNSC_selected_patches\"\n",
    "\n",
    "for p in Path(renji_pth).rglob(\"*_mask.nii.gz\"):\n",
    "    basename = osp.basename(p).replace(\"_mask.nii.gz\", \"\")\n",
    "    patient = basename.split('_ins')[0]\n",
    "    ln_id = int(basename.split('_ins')[-1].split('_')[0])\n",
    "    if '_pos' in basename:\n",
    "        sad = name_to_sad[basename.lower().split('_pos')[0]]\n",
    "    else:\n",
    "        sad = name_to_sad[basename.lower().split('_neg')[0]]\n",
    "    \n",
    "    if patient not in renji_ln_count:\n",
    "        renji_ln_count[patient] = []\n",
    "        renji_ln_rm[patient] = []\n",
    "    \n",
    "    if sad >= 5 or '_pos' in basename:\n",
    "        renji_ln_count[patient].append(ln_id)\n",
    "    else:\n",
    "        renji_ln_rm[patient].append(ln_id)\n",
    "\n",
    "for p in Path(cgmh_pth).rglob(\"*_mask.nii.gz\"):\n",
    "    basename = osp.basename(p).replace(\"_mask.nii.gz\", \"\")\n",
    "    patient = basename.split('_ins')[0]\n",
    "    ln_id = int(basename.split('_ins')[-1].split('_')[0])\n",
    "    if '_pos' in basename:\n",
    "        sad = name_to_sad[basename.lower().split('_pos')[0]]\n",
    "    else:\n",
    "        sad = name_to_sad[basename.lower().split('_neg')[0]]\n",
    "    \n",
    "    if patient not in cgmh_ln_count:\n",
    "        cgmh_ln_count[patient] = []\n",
    "        cgmh_ln_rm[patient] = []\n",
    "    \n",
    "    if sad >= 5 or '_pos' in basename:\n",
    "        cgmh_ln_count[patient].append(ln_id)\n",
    "    else:\n",
    "        cgmh_ln_rm[patient].append(ln_id)\n",
    "    \n",
    "for p in Path(tcga_pth).rglob(\"*_mask.nii.gz\"):\n",
    "    basename = osp.basename(p).replace(\"_mask.nii.gz\", \"\")\n",
    "    patient = basename.split('_ins')[0]\n",
    "    ln_id = int(basename.split('_ins')[-1].split('_')[0])\n",
    "    if '_pos' in basename:\n",
    "        sad = name_to_sad[basename.lower().split('_pos')[0]]\n",
    "    else:\n",
    "        sad = name_to_sad[basename.lower().split('_neg')[0]]\n",
    "    \n",
    "    if patient not in tcga_ln_count:\n",
    "        tcga_ln_count[patient] = []\n",
    "        tcga_ln_rm[patient] = []\n",
    "    \n",
    "    if sad >= 5 or '_pos' in basename:\n",
    "        tcga_ln_count[patient].append(ln_id)\n",
    "    else:\n",
    "        tcga_ln_rm[patient].append(ln_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renji_anno_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/Fudan_HN_LN_20231204/headNeck_LN_label_infor(7).csv\"\n",
    "cgmh_anno_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/CGMH/headNeck_LN_label_infor.csv\"\n",
    "tcga_anno_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/TCGA/headNeck_LN_label_infor.csv\"\n",
    "df_renji = pd.read_csv(renji_anno_pth)\n",
    "df_cgmh = pd.read_csv(cgmh_anno_pth)\n",
    "df_tcga = pd.read_csv(tcga_anno_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reader 1 and reader 2's annotation\n",
    "rd1_renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader1_renji_remapped.csv\"\n",
    "rd1_cgmh_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader1_cgmh.csv\"\n",
    "rd1_tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader1_tcga_remapped.csv\"\n",
    "df_rd1_renji = pd.read_csv(rd1_renji_pth)\n",
    "df_rd1_cgmh = pd.read_csv(rd1_cgmh_pth)\n",
    "df_rd1_tcga = pd.read_csv(rd1_tcga_pth)\n",
    "\n",
    "rd2_renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader2_renji_remapped.csv\"\n",
    "rd2_cgmh_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader2_cgmh.csv\"\n",
    "rd2_tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader2_tcga_remapped.csv\"\n",
    "df_rd2_renji = pd.read_csv(rd2_renji_pth)\n",
    "df_rd2_cgmh = pd.read_csv(rd2_cgmh_pth)\n",
    "df_rd2_tcga = pd.read_csv(rd2_tcga_pth)\n",
    "\n",
    "rd3_renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader3_sn_renji_remapped.csv\"\n",
    "rd3_cgmh_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader3_sn_cgmh.csv\"\n",
    "rd3_tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader3_sn_tcga_remapped.csv\"\n",
    "df_rd3_renji = pd.read_csv(rd3_renji_pth)\n",
    "df_rd3_cgmh = pd.read_csv(rd3_cgmh_pth)\n",
    "df_rd3_tcga = pd.read_csv(rd3_tcga_pth)\n",
    "\n",
    "rd4_renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader4_dd_renji_remapped.csv\"\n",
    "rd4_cgmh_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader4_dd_cgmh.csv\"\n",
    "rd4_tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader4_dd_tcga_remapped.csv\"\n",
    "df_rd4_renji = pd.read_csv(rd4_renji_pth)\n",
    "df_rd4_cgmh = pd.read_csv(rd4_cgmh_pth)\n",
    "df_rd4_tcga = pd.read_csv(rd4_tcga_pth)\n",
    "\n",
    "rd5_renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader5_xbb_renji_remapped.csv\"\n",
    "rd5_cgmh_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader5_xbb_cgmh.csv\"\n",
    "rd5_tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader5_xbb_tcga_remapped.csv\"\n",
    "df_rd5_renji = pd.read_csv(rd5_renji_pth)\n",
    "df_rd5_cgmh = pd.read_csv(rd5_cgmh_pth)\n",
    "df_rd5_tcga = pd.read_csv(rd5_tcga_pth)\n",
    "\n",
    "rd6_renji_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader6_albert_renji_remapped.csv\"\n",
    "rd6_cgmh_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader6_albert_cgmh.csv\"\n",
    "rd6_tcga_pth = \"./datasets/LN_classify/Fudan_HN_LN_22-23_all/ReaderStudy/reader6_albert_tcga_remapped.csv\"\n",
    "df_rd6_renji = pd.read_csv(rd6_renji_pth)\n",
    "df_rd6_cgmh = pd.read_csv(rd6_cgmh_pth)\n",
    "df_rd6_tcga = pd.read_csv(rd6_tcga_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rater_results(df_reader, df_anno, dataset):\n",
    "    rater_met_res = []\n",
    "    rater_ene_res = []\n",
    "    read_order = []\n",
    "\n",
    "    for idx, reader_row in df_reader.iterrows():\n",
    "        patient = reader_row[\"file_name\"].replace(\".nii.gz\", \"\")\n",
    "        ln_id2idx = {}\n",
    "        if dataset == 'renji':\n",
    "            patient_annotations = len(renji_ln_count[patient])\n",
    "            all_lns = set(renji_ln_count[patient])\n",
    "            sorted_ids = sorted(renji_ln_count[patient])\n",
    "            for idx, ln_id in enumerate(sorted_ids):\n",
    "                ln_id2idx[ln_id] = idx\n",
    "        elif dataset == 'cgmh':\n",
    "            if 'CGMH_Larynx20240117_anon_025' in patient:\n",
    "                continue\n",
    "            patient_annotations = len(cgmh_ln_count[patient])\n",
    "            all_lns = set(cgmh_ln_count[patient])\n",
    "            sorted_ids = sorted(cgmh_ln_count[patient])\n",
    "            for idx, ln_id in enumerate(sorted_ids):\n",
    "                ln_id2idx[ln_id] = idx\n",
    "        elif dataset == 'tcga':\n",
    "            patient_annotations = len(tcga_ln_count[patient])\n",
    "            all_lns = set(tcga_ln_count[patient])\n",
    "            sorted_ids = sorted(tcga_ln_count[patient])\n",
    "            for idx, ln_id in enumerate(sorted_ids):\n",
    "                ln_id2idx[ln_id] = idx\n",
    "        else:\n",
    "            raise ValueError(\"Invalid type\")\n",
    "            \n",
    "        row_meta_preds = reader_row[\"positive_LN_id\"]\n",
    "        if isinstance(row_meta_preds, pd.Series):\n",
    "            row_meta_preds = row_meta_preds.values[0]\n",
    "        \n",
    "        if isinstance(row_meta_preds, float) and np.isnan(row_meta_preds):\n",
    "            meta_preds = []\n",
    "        elif isinstance(row_meta_preds, (int, float)):\n",
    "            meta_preds = [int(row_meta_preds)]\n",
    "        elif isinstance(row_meta_preds, str) and row_meta_preds != \"无转移\" and row_meta_preds != \"\":\n",
    "            meta_preds = [int(p) for p in row_meta_preds.rstrip(',').split(\",\")]\n",
    "        elif row_meta_preds == \"无转移\" or row_meta_preds == \"\":\n",
    "            meta_preds = []\n",
    "        else:\n",
    "            raise ValueError(\"No predicted found\")\n",
    "        \n",
    "        # remove not selected LNs (e.g., <5mm negative and >LIMIT ENEs)\n",
    "        if dataset == 'renji':\n",
    "            meta_preds = [p for p in meta_preds if p not in renji_ln_rm[patient]]\n",
    "        elif dataset == 'cgmh':\n",
    "            meta_preds = [p for p in meta_preds if p not in cgmh_ln_rm[patient]]\n",
    "        elif dataset == 'tcga':\n",
    "            meta_preds = [p for p in meta_preds if p not in tcga_ln_rm[patient]]\n",
    "        \n",
    "        row_ene_preds = reader_row[\"ENE_LN_id\"]\n",
    "        if isinstance(row_ene_preds, pd.Series):\n",
    "            row_ene_preds = row_ene_preds.values[0]\n",
    "          \n",
    "        if isinstance(row_ene_preds, float) and np.isnan(row_ene_preds):\n",
    "            ene_preds = []\n",
    "        elif isinstance(row_ene_preds, (int, float)):\n",
    "            ene_preds = [int(row_ene_preds)]\n",
    "        elif isinstance(row_ene_preds, str):\n",
    "            ene_preds = [int(p) for p in row_ene_preds.rstrip(',').split(\",\")]\n",
    "        else:\n",
    "            ene_preds = []\n",
    "        \n",
    "        # remove not selected LNs (e.g., <5mm negative and >LIMIT ENEs)\n",
    "        if dataset == 'renji':\n",
    "            ene_preds = [p for p in ene_preds if p not in renji_ln_rm[patient]]\n",
    "        elif dataset == 'cgmh':\n",
    "            ene_preds = [p for p in ene_preds if p not in cgmh_ln_rm[patient]]\n",
    "        elif dataset == 'tcga':\n",
    "            ene_preds = [p for p in ene_preds if p not in tcga_ln_rm[patient]]\n",
    "        \n",
    "        anno_row = df_anno[df_anno[\"file_name\"] == patient]\n",
    "        if anno_row.empty:\n",
    "            print(\"{} is not matched any annotation\".format(patient))\n",
    "            continue\n",
    "        \n",
    "        row_anno_meta = anno_row[\"positive_LN_id\"]\n",
    "        if isinstance(row_anno_meta, pd.Series):\n",
    "            row_anno_meta = row_anno_meta.values[0]\n",
    "        \n",
    "        if isinstance(row_anno_meta, float) and np.isnan(row_anno_meta):\n",
    "            anno_meta = []\n",
    "        elif isinstance(row_anno_meta, (int, float)):\n",
    "            anno_meta = [int(row_anno_meta)]\n",
    "        elif isinstance(row_anno_meta, str) and row_anno_meta != \"无转移\":\n",
    "            anno_meta = [int(p) for p in row_anno_meta.rstrip(',').split(\",\")]\n",
    "        elif row_anno_meta == \"无转移\":\n",
    "            anno_meta = []\n",
    "        else:\n",
    "            raise ValueError(\"No annotation found\")\n",
    "        \n",
    "        # remove not selected LNs (e.g., <5mm negative and >LIMIT ENEs)\n",
    "        if dataset == 'renji':\n",
    "            anno_meta = [p for p in anno_meta if p not in renji_ln_rm[patient]]\n",
    "        elif dataset == 'cgmh':\n",
    "            anno_meta = [p for p in anno_meta if p not in cgmh_ln_rm[patient]]\n",
    "        elif dataset == 'tcga':\n",
    "            anno_meta = [p for p in anno_meta if p not in tcga_ln_rm[patient]]\n",
    "        \n",
    "        row_anno_ene = anno_row[\"ENE_LN_id\"]\n",
    "        if isinstance(row_anno_ene, pd.Series):\n",
    "            row_anno_ene = row_anno_ene.values[0]\n",
    "          \n",
    "        if isinstance(row_anno_ene, float) and np.isnan(row_anno_ene):\n",
    "            anno_ene = []\n",
    "        elif isinstance(row_anno_ene, (int, float)):\n",
    "            anno_ene = [int(row_anno_ene)]\n",
    "        elif isinstance(row_anno_ene, str):\n",
    "            anno_ene = [int(p) for p in row_anno_ene.rstrip(',').split(\",\")]\n",
    "        else:\n",
    "            anno_ene = []\n",
    "        \n",
    "        # remove not selected LNs (e.g., <5mm negative and >LIMIT ENEs)\n",
    "        if dataset == 'renji':\n",
    "            anno_ene = [p for p in anno_ene if p not in renji_ln_rm[patient]]\n",
    "        elif dataset == 'cgmh':\n",
    "            anno_ene = [p for p in anno_ene if p not in cgmh_ln_rm[patient]]\n",
    "        elif dataset == 'tcga':\n",
    "            anno_ene = [p for p in anno_ene if p not in tcga_ln_rm[patient]]\n",
    "\n",
    "            \n",
    "        meta_pred_idx = set([ln_id2idx[p] for p in meta_preds if p in all_lns])\n",
    "        ene_preds_idx = set([ln_id2idx[p] for p in ene_preds if p in all_lns])\n",
    "\n",
    "        # if len(all_lns) < max(all_lns):\n",
    "        #     a = 0\n",
    "        # anno_meta = set(anno_meta)\n",
    "        # anno_ene = set(anno_ene)\n",
    "        \n",
    "        # prepare for inter-rater reliability\n",
    "        read_order.append(patient)\n",
    "        meta_read_res = np.zeros(patient_annotations).astype(int)\n",
    "        ene_read_res = np.zeros(patient_annotations).astype(int)\n",
    "        meta_read_res[list(meta_pred_idx)] = 1\n",
    "        ene_read_res[list(ene_preds_idx)] = 1\n",
    "\n",
    "        rater_met_res.append(meta_read_res)\n",
    "        rater_ene_res.append(ene_read_res)\n",
    "    \n",
    "    ordered_rater_met = [rater_met_res[i] for i in np.argsort(read_order)]\n",
    "    ordered_rater_ene = [rater_ene_res[i] for i in np.argsort(read_order)]\n",
    "    rater_met_res = np.concatenate(ordered_rater_met, axis=0)\n",
    "    rater_ene_res = np.concatenate(ordered_rater_ene, axis=0)\n",
    "\n",
    "    return rater_met_res, rater_ene_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 5)\n",
      "Fudan EENT Meta Fleiss Kappa:  0.43365783895228965\n",
      "Fudan EENT ENE Fleiss Kappa:  0.500185585099204\n"
     ]
    }
   ],
   "source": [
    "# EENT\n",
    "renji_r1_met_rate, renji_r1_ene_rate = generate_rater_results(df_rd1_renji, df_renji, dataset='renji')\n",
    "renji_r2_met_rate, renji_r2_ene_rate = generate_rater_results(df_rd2_renji, df_renji, dataset='renji')\n",
    "renji_r4_met_rate, renji_r4_ene_rate = generate_rater_results(df_rd4_renji, df_renji, dataset='renji')\n",
    "renji_r5_met_rate, renji_r5_ene_rate = generate_rater_results(df_rd5_renji, df_renji, dataset='renji')\n",
    "renji_r6_met_rate, renji_r6_ene_rate = generate_rater_results(df_rd6_renji, df_renji, dataset='renji')\n",
    "\n",
    "renji_met_rater_all = np.stack([renji_r1_met_rate, renji_r2_met_rate, renji_r4_met_rate, renji_r5_met_rate, renji_r6_met_rate], axis=1)\n",
    "print(renji_met_rater_all.shape)\n",
    "intermediate = aggregate_raters(renji_met_rater_all)\n",
    "renji_met_fleiss_kappa = irr.fleiss_kappa(intermediate[0], \"fleiss\")\n",
    "print(\"Fudan EENT Meta Fleiss Kappa: \", renji_met_fleiss_kappa)\n",
    "\n",
    "renji_ene_rater_all = np.stack([renji_r1_ene_rate, renji_r2_ene_rate, renji_r4_ene_rate, renji_r5_ene_rate, renji_r6_ene_rate], axis=1)\n",
    "intermediate = aggregate_raters(renji_ene_rater_all)\n",
    "renji_ene_fleiss_kappa = irr.fleiss_kappa(intermediate[0], \"fleiss\")\n",
    "print(\"Fudan EENT ENE Fleiss Kappa: \", renji_ene_fleiss_kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 5)\n",
      "CGMH Meta Fleiss Kappa:  0.36882068758894443\n",
      "CGMH ENE Fleiss Kappa:  0.332389878742885\n"
     ]
    }
   ],
   "source": [
    "# CGMH\n",
    "cgmh_r1_met_rate, cgmh_r1_ene_rate = generate_rater_results(df_rd1_cgmh, df_cgmh, dataset='cgmh')\n",
    "cgmh_r2_met_rate, cgmh_r2_ene_rate = generate_rater_results(df_rd2_cgmh, df_cgmh, dataset='cgmh')\n",
    "cgmh_r4_met_rate, cgmh_r4_ene_rate = generate_rater_results(df_rd4_cgmh, df_cgmh, dataset='cgmh')\n",
    "cgmh_r5_met_rate, cgmh_r5_ene_rate = generate_rater_results(df_rd5_cgmh, df_cgmh, dataset='cgmh')\n",
    "cgmh_r6_met_rate, cgmh_r6_ene_rate = generate_rater_results(df_rd6_cgmh, df_cgmh, dataset='cgmh')\n",
    "\n",
    "\n",
    "cgmh_met_rater_all = np.stack([cgmh_r1_met_rate, cgmh_r2_met_rate, cgmh_r4_met_rate, cgmh_r5_met_rate, cgmh_r6_met_rate], axis=1)\n",
    "print(cgmh_met_rater_all.shape)\n",
    "intermediate = aggregate_raters(cgmh_met_rater_all)\n",
    "cgmh_met_fleiss_kappa = irr.fleiss_kappa(intermediate[0], \"fleiss\")\n",
    "print(\"CGMH Meta Fleiss Kappa: \", cgmh_met_fleiss_kappa)\n",
    "\n",
    "cgmh_ene_rater_all = np.stack([cgmh_r1_ene_rate, cgmh_r2_ene_rate, cgmh_r4_ene_rate, cgmh_r5_ene_rate, cgmh_r6_ene_rate], axis=1)\n",
    "intermediate = aggregate_raters(cgmh_ene_rater_all)\n",
    "cgmh_ene_fleiss_kappa = irr.fleiss_kappa(intermediate[0], \"fleiss\")\n",
    "print(\"CGMH ENE Fleiss Kappa: \", cgmh_ene_fleiss_kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 5)\n",
      "TCGA-TCIA Meta Fleiss Kappa:  0.3691068018289795\n"
     ]
    }
   ],
   "source": [
    "# TCGA\n",
    "tcga_r1_met_rate, tcga_r1_ene_rate = generate_rater_results(df_rd1_tcga, df_tcga, dataset='tcga')\n",
    "tcga_r2_met_rate, tcga_r2_ene_rate = generate_rater_results(df_rd2_tcga, df_tcga, dataset='tcga')\n",
    "tcga_r4_met_rate, tcga_r4_ene_rate = generate_rater_results(df_rd4_tcga, df_tcga, dataset='tcga')\n",
    "tcga_r5_met_rate, tcga_r5_ene_rate = generate_rater_results(df_rd5_tcga, df_tcga, dataset='tcga')\n",
    "tcga_r6_met_rate, tcga_r6_ene_rate = generate_rater_results(df_rd6_tcga, df_tcga, dataset='tcga')\n",
    "\n",
    "tcga_met_rater_all = np.stack([tcga_r1_met_rate, tcga_r2_met_rate, tcga_r4_met_rate, tcga_r5_met_rate, tcga_r6_met_rate], axis=1)\n",
    "print(tcga_met_rater_all.shape)\n",
    "intermediate = aggregate_raters(tcga_met_rater_all)\n",
    "tcga_met_fleiss_kappa = irr.fleiss_kappa(intermediate[0], \"fleiss\")\n",
    "print(\"TCGA-TCIA Meta Fleiss Kappa: \", tcga_met_fleiss_kappa)\n",
    "\n",
    "# tcga_ene_rater_all = np.stack([tcga_r1_ene_rate, tcga_r2_ene_rate, tcga_r4_ene_rate, tcga_r5_ene_rate, tcga_r6_ene_rate], axis=1)\n",
    "# intermediate = aggregate_raters(tcga_ene_rater_all)\n",
    "# tcga_ene_fleiss_kappa = irr.fleiss_kappa(intermediate[0], \"fleiss\")\n",
    "# print(\"TCGA-TCIA ENE Fleiss Kappa: \", tcga_ene_fleiss_kappa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohen's Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renji_annotations = renji_met_rater_all.transpose()\n",
    "num_annotators = renji_annotations.shape[0]\n",
    "\n",
    "# List to store pairwise Cohen's Kappa scores\n",
    "kappa_scores = []\n",
    "\n",
    "# Calculate Cohen's Kappa for each pair of annotators\n",
    "for i in range(num_annotators):\n",
    "    for j in range(i + 1, num_annotators):\n",
    "        kappa = cohen_kappa_score(renji_annotations[i], renji_annotations[j])\n",
    "        kappa_scores.append(kappa)\n",
    "\n",
    "# Calculate the average Cohen's Kappa\n",
    "average_kappa = np.mean(kappa_scores)\n",
    "\n",
    "print(f'EENT pairwise Cohen\\'s Kappa scores: {kappa_scores}')\n",
    "print(f'EENT Average Cohen\\'s Kappa: {average_kappa}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtnet_py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
